# Breast Cancer (Wisconsin) – XGBoost vs Random Forest (with Grid Search)

This project builds and evaluates two binary classifiers — **XGBoost** and **Random Forest** — to predict whether a breast tumor is **malignant (M)** or **benign (B)** from features computed on digitized images of fine-needle aspiration (FNA) biopsies. The dataset is a classic benchmark used widely in machine learning and medical imaging research.

## Dataset
This dataset contains features computed from digitized images of breast tissue taken during a fine-needle aspiration (FNA) biopsy. The goal is to classify whether a tumor is malignant (cancerous) or benign (non-cancerous) based on the shape and texture of the cell nuclei present in the image.

It is widely used in machine learning and medical imaging research and is a benchmark dataset for binary classification tasks.

**Assumptions in this repo**
- The target column is named `diagnosis` with values `B` (benign) and `M` (malignant).
- There is a unique identifier column named `id` which we drop.
- All other columns are **numeric** features.
- There are **no missing values** and **no categorical features** requiring encoding.

> If your file uses different column names, adjust the constants at the top of the notebook.

## Models & Regularization
We compare:
- **XGBoost**: Hyperparameters tuned with `GridSearchCV`. L2 regularization via `reg_lambda`.
- **Random Forest**: Hyperparameters tuned with `GridSearchCV`. While Random Forests do not use an explicit L2 penalty, we regularize via **cost-complexity pruning** (`ccp_alpha`) and structure controls (`min_samples_leaf`, `min_samples_split`, `max_features`, `max_depth`). We optionally search `class_weight` to mitigate class imbalance.

## Why Random Forest can sometimes beat XGBoost
On relatively **small datasets** (≈1k rows), variance in performance estimates can be high. Bagging methods like Random Forest reduce variance through averaging many trees and can perform very competitively — sometimes better than boosting — especially if boosting is not aggressively tuned for recall on the minority class.

## What the notebook does
- Loads and cleans the data (drops `id`, encodes `B`→0 and `M`→1).
- Splits data into **stratified** train/test sets.
- Computes an initial **class weight ratio** for XGBoost via `scale_pos_weight`.
- Performs **GridSearchCV** for XGBoost and Random Forest with **5-fold Stratified CV**.
- Reports **Precision, Recall, Accuracy, and F1** for the **test set**.
- Also reports **cross-validated** metrics for a fairer comparison.
- Saves a **results table** to CSV for quick comparison.

## Getting Started

### Environment
```bash
pip install -U scikit-learn xgboost matplotlib pandas numpy
```

### Run
1. Place your dataset as `your_data.csv` in the project root (or set a different path at the top of the notebook).
2. Open the notebook and run all cells.
3. Review the printed metrics and `model_comparison.csv`.

## File Structure
- `README.md` – this file
- `breast_cancer_xgb_rf.ipynb` – end-to-end, modular notebook
- (You provide) `your_data.csv` – your dataset file

## License
MIT (or your choice).

## Acknowledgments
- Breast Cancer Wisconsin dataset authors and the ML community for maintaining this benchmark.
